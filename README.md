# ğŸ“Š AnÃ¡lise de Vendas de uma Livraria com PySpark no Databricks

Este projeto realiza a anÃ¡lise de um arquivo de vendas de uma livraria utilizando **Apache Spark (PySpark)** no ambiente **Databricks Community Edition**.

---

## ğŸš€ Tecnologias Utilizadas

- **Python 3.5 (com PySpark)**  
- **Apache Spark**: Processamento distribuÃ­do de dados em larga escala  
- **Databricks Community Edition**: Plataforma gratuita com notebooks colaborativos  
- **Unittest**: Framework de testes unitÃ¡rios do Python  

---

## âš™ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Criar Conta no Databricks Community Edition

- Acesse: [Databricks Community](https://community.cloud.databricks.com/)
- Crie uma conta gratuita

### 2. Importar os Arquivos

- VÃ¡ atÃ©: `Workspace > [seu usuÃ¡rio] > Create > Folder`
- Crie uma pasta chamada **`vendas_livraria`**
- Dentro dela, importe os seguintes arquivos:
  - `vendas_livraria.ipynb`
  - (Opcional) `test_vendas_livraria.py.ipynb` â€“ caso deseje rodar os testes unitÃ¡rios

### 3. Criar a Tabela no Databricks

- Abra o notebook `vendas_livraria`
- VÃ¡ em `File > Add Data > Create or Modify Table`
- Importe o arquivo `vendas_livros.csv`
- Aguarde o preview da tabela e clique em **"Create Table"**

---

## â–¶ï¸ Como Executar o Projeto

Abra o notebook `vendas_livraria.ipynb` e:

1. Altere o caminho do arquivo, se necessÃ¡rio  
   Ex: `workspace.default.vendas_livros`
2. Execute todas as cÃ©lulas para:
   - Carregar os dados  
   - Rodar os testes de qualidade da tabela  
   - Criar a tabela `resumo_vendas_diarias`  
   - Descobrir o autor mais vendido  
   - Identificar a data campeÃ£ de vendas  

---

## ğŸ§ª Executando os Testes

- Crie um notebook separado com o conteÃºdo de `test_vendas_livraria.py.ipynb`
- Execute o notebook
- Os testes verificarÃ£o:
  - Leitura correta da tabela  
  - Schema correto  
  - MÃ­nimo de 10 datas distintas  
  - AusÃªncia de datas futuras  
  - Se existem IDs negativos  

---

## ğŸ› ï¸ Justificativas TÃ©cnicas

- **Apache Spark com PySpark**: Escolhido pela escalabilidade e eficiÃªncia para processar grandes volumes de dados  
- **Databricks Community Edition**: Permite desenvolvimento com Spark gratuitamente, com ambiente interativo e gerenciado  
- **Unittest**: Leve, robusto e integrado ao Python, facilita a validaÃ§Ã£o e reprodutibilidade  
- **Arquitetura MedalhÃ£o**: Utilizada para organizar e validar dados em camadas, garantindo integridade e governanÃ§a (ACID)

---

## ğŸ“ Notas

Durante o desenvolvimento, busquei aplicar:

- Boas prÃ¡ticas de codificaÃ§Ã£o com PySpark  
- Uma estrutura reutilizÃ¡vel e escalÃ¡vel  
- Testes unitÃ¡rios para validar transformaÃ§Ãµes crÃ­ticas

---

## ğŸ”® Melhorias Futuras

- Criar um pipeline de monitoramento contÃ­nuo com agendamento automÃ¡tico  
- Testar tambÃ©m as funÃ§Ãµes de transformaÃ§Ã£o e as queries SQL de qualidade  

---

## ğŸ™ Obrigado!

Este projeto mostrou como o **Databricks Community Edition** pode ser uma excelente porta de entrada para trabalhar com **Spark** de forma prÃ¡tica e gratuita â€” ideal para quem estÃ¡ aprendendo ou prototipando soluÃ§Ãµes em larga escala.
